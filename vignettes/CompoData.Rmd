---
title: "Analysis of compositional data"
author: "Emanuel Heitlinger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analysis of compositional data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Analysis of compositional count data 
## (e.g. in microbiomes or transcriptomes)

Standard analyses of abundance estimates genereated from sequencing
treat the generated data as counts. Methods for differential abundance
estimation in RNAseq data have probably recieved most attention in
this regard
([see e.g. this for one of the current approaches](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0550-8)). Many
(if not all) of the methods developed for analyses of count data
derived from transcriptomes are also applied to amplicon sequencing
data. On the other hand techniques from classical community ecolgy are
applied on data from transcriptomics or amplicon sequencing alike
(mostly after _transforming_ it to count data). 

In the field of amplicon sequencing there are currently concernes that
analysis of "compositional data" as counts can introduce biases ([e.g. reviewd here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5695134/), refering in its core to [Aitchison, J. (1986)](https://www.amazon.de/Statistical-Compositional-Monographs-Statistics-Probability/dp/0412280604)). 

This document aims to give an introduction to the challenges posed by
compositional data produced to assess transcriptomes or microbiomes. I
will give an "abstract" introduction into:

1. pecularities of "compositional count data" from sequencing, 

and show

2. how analyses of community ecology (try to) deal with these. 

The main purpose is to discuss with the audience how concepts in
community ecology and data analyses are affected by the nature of
sequencing data.

It is distributed and "developed" as a package to ease further
expansion and contribution

## So what is this "compositional data"?

Imagine you would count animals in two different ecosystems (e.g. on
mars, earth and venus) but at the end forget your overall count. What
you keep is just the relative proporion of tigers, unicorns and
ladybugs.

Why should such a strange thing happen? 

When sequencing DNA fragments, we sample a pool of molecules. We
usually lack the information on the number of entities (organisms or
transcripts) this pool of molecues was generated from
([but see this as a counter example](https://www.nature.com/articles/nature24460)).

To illustrate we start to simulate some count data (strongly inspired
by `DESeq2::makeExampleDESeqDataSet`)

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
```


```{r, fig.height=4, fig.show='hold', fig.width=4}
library(reshape)
## initialize the random number generator (-> reproducibility)
set.seed(1209)

n <- 3 ## number of rows
m <- 3 ## number of columns

means <- c(8, 28, 330) ## per row (species) mean

countData <- matrix(rnbinom(m * n, mu = means, size = 10), 
                    ncol = m)

colnames(countData) <- c("venus", "earth", "mars")
rownames(countData) <- c("tigers", "unicorns", "ladybugs")

ggplot(melt(countData),
       aes(x=X2, y=value, fill=X1)) +
    geom_col()
```

In addition our sampling of the starting molecules usually varies in
depth. A usual first step in all analyses is to account for this
variation in sampling depths. Standard (presented below) methods
don't consider the compositional nature of sequencing data when dong
so.


```{r, fig.show='hold', fig.height=4, fig.width=4}
depth <- c(1, 0.1, 0.5) ## per column (planet) depth
mu <- as.matrix(means)%*%t(as.matrix(depth))

countData <- matrix(rnbinom(m * n, mu = mu, size = 10), 
                   ncol = m)
colnames(countData) <- c("venus", "earth", "mars")
rownames(countData) <- c("tigers", "unicorns", "ladybugs")

ggplot(melt(countData),
       aes(x=X2, y=value, fill=X1)) +
    geom_col()
```

## Scaling by totals (1)

The simplest method to adjust for the depth of sequencing is to scale
by the number of total reads per sample.

```{r, results='as.is', fig.height=4, fig.show='hold', fig.width=4}
scaling.factor <- colSums(countData)/max(colSums(countData))

ggplot(melt(t(t(countData)*1/scaling.factor)),
       aes(x=X2, y=value, fill=X1)) +
    geom_col()
```

## Interlude: A more realistic dataset

We adjust a couple of things in our dataset to make it more realistic:

1. We add more species (rows, could also be genes in trascriptomics)

2. We add more samples (columns) and also specify them as replicates
   for our differnt ecosystmes (could be tissues or treatments in
   transcriptomics)

3. Differences between ecosystems

4. We make the dispersion (negative binomial parameter,
   a.k.a. parameter of the gamma distribution describing the variation
   in the poisson paramter of a composite poisson-gamma) vary with the
   mean of the count for a species.


```{r, fig.height=4, fig.show='hold', fig.width=4}
library(pheatmap)
n <- 300 ## number of rows (species)
m <- 36 ## number of columns (samples)

## the relationship between mean and dispersion
dispMeanRel <- function(x) 4/x + 0.1

## low mean and high standard deviation to get sparse counts (lots of
## zeros)
interceptMean <- 0.1
interceptSD <- 2

## the depth as influenced by our sequencing
## depth <- runif(m, min=0.4, max=1)

##  for now assume we have even depths
depth <- rep(1, times=m)

## beta will be the mean of the negative binomial it is here still on
## the log scale we make two more columns wich give a deviation from
## our intercept SD (see below)
beta <- cbind(rnorm(n, interceptMean, interceptSD),
              rnorm(n, 0, 1),
              rnorm(n, 0, 2))

## the sample different ecosystems
colData <- data.frame(planet =
                          factor(rep(c("venus", "earth", "mars"),
                                     each = m/3)))
rownames(colData) <- paste(colData$planet, 1:m, sep="_")

## a model matrix telling us which sample belongs to which condition
planet.design <- stats::model.matrix.default(~colData$planet)

## the final sample means based on design, log-means and the depth of
## sequencing. By matrix multiplication mars gets 
mu <- t(2^(planet.design %*% t(beta)) * depth)

## the dispersion via the mean relationship 
dispersion <- dispMeanRel(2^(beta[, 1]))

## and based of on these the negative binomial counts
countData <- matrix(rnbinom(m * n, mu = mu, size = dispersion), 
                    ncol = m)

## we now remove species for which we have only zero counts (we
## wouldn't have this in a real dataset)
countData <- countData[rowSums(countData)>0, ]

colnames(countData) <- rownames(colData)
rownames(countData) <- paste("species", 1:nrow(countData), sep="_")

pheatmap(log10(countData+1),
         annotation_col=colData,
         show_rownames=FALSE,
         show_colnames=FALSE)
```

## Scaling by totals (2, slightly more realistic data)

```{r, results='as.is', fig.height=4, fig.show='hold', fig.width=4}

ColS <- data.frame(sample.sum=colSums(countData), num=1:ncol(countData),
                   planet=colData$planet)

ggplot(ColS, aes(x=reorder(num, sample.sum), y=sample.sum, fill=planet))+
    geom_bar(stat = "identity")

scaling.factor <- colSums(countData)/max(colSums(countData))

countData.scaled <- t(t(countData)*1/scaling.factor)

pheatmap(log10(countData.scaled+1),
         annotation_col=colData,
         show_rownames=FALSE,
         show_colnames=FALSE)
```

Interestingly, scaleing by totals seems to screw us over here. We did
not include sequencing depth differnences in this simulation, so we
know we wouldn't have needed to scale. In real data we don't know this
and we have to correct for the differences in sequencing depths.

## Rarefaction

What if we wanted to estimate species richness? For any kind of basic
species count or for the richness indices we need to correct for our
observation effort!

This means we need to sub-sample every sample to the number of
sequencing reads we obtained for the least deeply assessed sample we
want to include. We do this repeatedly to get an estimate of the
variance this induces.

We use a the `vegan` function to perform this rarefication. Ecologists
traditionally have put the species in the columns and sites in the
rows, probably as they had more sites than spcies. We have in most
cases more species than sites and do it the other way round (at least
I do).

```{r, results='as.is', fig.height=4, fig.show='hold', fig.width=4}
library(vegan)
countData.rare <- lapply(1:100, function (i){
    t(rrarefy(t(countData),
              sample=min(ColS$sample.sum)))
})

pheatmap(log10(countData.rare[[1]]+1),
         annotation_col=colData,
         show_rownames=FALSE,
         show_colnames=FALSE)

richness <- lapply(countData.rare, function(y){
    apply(y, 2, function(x) length(x[x>0]))
})

rich.mat <- matrix(unlist(richness), nrow=m)

richness <- data.frame(planet=colData$planet,
                       richness.mean=rowMeans(rich.mat),
                       richness.sd=apply(rich.mat, 1, sd),
                       n=1:nrow(rich.mat))

ggplot(richness, aes(as.character(n), richness.mean)) +
    geom_point(size=2) +
    geom_errorbar(aes(ymin=richness.mean-richness.sd,
                      ymax=richness.mean+richness.sd))+
    facet_wrap(~planet, scales="free_x")
```

Uh, we are screwed again!

## Using totals based scaling as offset for modelling

We will skip this...

## Techniques acknowleding compositionality

Centered log-ratio (clr) transformation offers an alternative
([e.g. reviewd here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5695134/)). 

```{r, results='as.is', fig.height=4, fig.show='hold', fig.width=4}
library(ALDEx2)
g.mean <- function (x, na.rm=TRUE){
    exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}

countData.clr <- apply(countData, 2, function(x){
    log(x+0.0001/g.mean(x+0.0001))
})

pheatmap(countData.clr,
         annotation_col=colData,
         show_rownames=FALSE,
         show_colnames=FALSE)

countData.clrP <- aldex.clr(countData,
                            conds=as.character(colData$planet))


```

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
