---
title: "Analysis of compositional data"
author: "Emanuel Heitlinger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analysis of compositional data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Analysis of compositional data 
## (e.g. in microbiomes or transcriptomes)

This document aims to give an introduction to the challenges posed by
compositional data produced to assess transcriptomes or microbiomes. I
will give an "abstract" introduction into:

1. pecularities of "compositional count data" from sequencing, 

and show

2. how analyses of community ecology (try to) deal with these. 

The main purpose is to discuss with the audience how concepts in
community ecology and data analyses are affected by the nature of
sequencing data.

It is distributed and "developed" as a package to ease further
expansion and contribution

## So what is this "compositional data"?

Imagine you would count animals in two different ecosystems (e.g. on
mars, earth and venus) but at the end forget your overall count. What
you keep is just the relative proporion of tigers, unicorns and
ladybugs.

Why should such a strange thing happen? 

When sequencing DNA fragments, we sample a pool of molecules. We
usually lack the information on the number of entities (organisms or
transcripts) this pool of molecues was generated from
([but see this as a counter example](https://www.nature.com/articles/nature24460)).

To illustrate we start to simulate some count data (strongly inspired
by `DESeq2::makeExampleDESeqDataSet`)


```{r, fig.height=4, fig.show='hold', fig.width=4}
library(reshape)
library(ggplot2)

n <- 3 ## number of rows
m <- 3 ## number of columns

means <- c(3, 28, 330) ## per row (species) mean
depth <- c(0.1, 1, 0.2) ## per column (planet) depth
mu <- as.matrix(means)%*%t(as.matrix(depth))

set.seed(1209)
countData <- matrix(rnbinom(m * n, mu = means, size = 10), 
                    ncol = m)

colnames(countData) <- c("venus", "earth", "mars")
rownames(countData) <- c("tigers", "unicorns", "ladybugs")

ggplot(melt(countData),
       aes(x=X2, y=value, fill=X1)) +
    geom_col()

```



```{r}
## set.seed(1209)

## n <- 3 ## number of rows
## m <- 3 ## number of columns
## interceptMean <- 8
## interceptSD <- 3
## sizeFactors <- c(0.2, 1, 0.1)
    
## beta <- rnorm(n, interceptMean, interceptSD)

## ## we make the dispersion related to a mean
## dispMeanRel <- function(x) 4/x + 0.1 ## dispersion mean relationship
## dispersion <- dispMeanRel(2^beta)

## mu <- 2^beta*sizeFactors

## countData <- matrix(rnbinom(m * n, mu = mu, size = 1/dispersion), 
##                     ncol = m)
## colnames(countData) <- c("venus", "earth", "mars")
## rownames(countData) <- c("tigers", "unicorns", "ladybugs")

```


In addition our sampling of the starting molecules usually varies in
depth. A usual first step in all analyses is to account for this
variation in sampling depths. Standard (presented below) methods
don't consider the compositional nature of sequencing data when dong
so.

## Scaling by totals



## Using totals based scaling as offset for modelling

## Rarefaction

## Techniques acknowleding compositionality

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
